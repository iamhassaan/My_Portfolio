<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Walmart dataset analysis - An in-depth analysis on sales across several states using Machine Learning | Muhammad Hassaan Anwar</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="generator" content="Hugo 0.71.1" />
    
    
      <META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">
    

    
    
      <link href="/My_Portfolio/dist/css/app.1cb140d8ba31d5b2f1114537dd04802a.css" rel="stylesheet">
    

    

    
      
    

    
    
    <meta property="og:title" content="Walmart dataset analysis - An in-depth analysis on sales across several states using Machine Learning" />
<meta property="og:description" content="Walmart U.S dataset analysis - Predicting sales across states and stores. Analysis on a Kaggle Dataset (M5 Competition). This analysis was performed by me and my three group members : Gerardo Moreno, Jake Barrett and Jesus Sicairos Lopez" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://iamhassaan.github.io/My_Portfolio/post/project-1/" />
<meta property="article:published_time" content="2020-02-09T10:58:08-04:00" />
<meta property="article:modified_time" content="2020-02-09T10:58:08-04:00" />
<meta itemprop="name" content="Walmart dataset analysis - An in-depth analysis on sales across several states using Machine Learning">
<meta itemprop="description" content="Walmart U.S dataset analysis - Predicting sales across states and stores. Analysis on a Kaggle Dataset (M5 Competition). This analysis was performed by me and my three group members : Gerardo Moreno, Jake Barrett and Jesus Sicairos Lopez">
<meta itemprop="datePublished" content="2020-02-09T10:58:08-04:00" />
<meta itemprop="dateModified" content="2020-02-09T10:58:08-04:00" />
<meta itemprop="wordCount" content="2722">



<meta itemprop="keywords" content="scene," /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Walmart dataset analysis - An in-depth analysis on sales across several states using Machine Learning"/>
<meta name="twitter:description" content="Walmart U.S dataset analysis - Predicting sales across states and stores. Analysis on a Kaggle Dataset (M5 Competition). This analysis was performed by me and my three group members : Gerardo Moreno, Jake Barrett and Jesus Sicairos Lopez"/>

  </head>

  <body class="ma0 avenir bg-near-white">

    
   
  

  
  
  <header class="cover bg-top" style="background-image: url('https://iamhassaan.github.io/My_Portfolio/images/walmart.jpg');">
    <div class="pb3-m pb6-l bg-black-60">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="https://iamhassaan.github.io/My_Portfolio" class="f3 fw2 hover-white no-underline white-90 dib">
      Muhammad Hassaan Anwar
    </a>
    <div class="flex-l items-center">
      

      
        <ul class="pl0 mr3">
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/My_Portfolio/about/" title="About Me page">
              About Me
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/My_Portfolio/contact/" title="Contact page">
              Contact
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/My_Portfolio/post/" title="Projects page">
              Projects
            </a>
          </li>
          
        </ul>
      
      




<a href="https://twitter.com/iamthehassaan" target="_blank" class="link-transition twitter link dib z-999 pt3 pt0-l mr1" title="Twitter link" rel="noopener" aria-label="follow on Twitterâ€”â€”Opens in a new window">
  <svg height="32px"  style="enable-background:new 0 0 67 67;" version="1.1" viewBox="0 0 67 67" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M37.167,22.283c-2.619,0.953-4.274,3.411-4.086,6.101  l0.063,1.038l-1.048-0.127c-3.813-0.487-7.145-2.139-9.974-4.915l-1.383-1.377l-0.356,1.017c-0.754,2.267-0.272,4.661,1.299,6.271  c0.838,0.89,0.649,1.017-0.796,0.487c-0.503-0.169-0.943-0.296-0.985-0.233c-0.146,0.149,0.356,2.076,0.754,2.839  c0.545,1.06,1.655,2.097,2.871,2.712l1.027,0.487l-1.215,0.021c-1.173,0-1.215,0.021-1.089,0.467  c0.419,1.377,2.074,2.839,3.918,3.475l1.299,0.444l-1.131,0.678c-1.676,0.976-3.646,1.526-5.616,1.568  C19.775,43.256,19,43.341,19,43.405c0,0.211,2.557,1.397,4.044,1.864c4.463,1.377,9.765,0.783,13.746-1.568  c2.829-1.673,5.657-5,6.978-8.221c0.713-1.716,1.425-4.851,1.425-6.354c0-0.975,0.063-1.102,1.236-2.267  c0.692-0.678,1.341-1.419,1.467-1.631c0.21-0.403,0.188-0.403-0.88-0.043c-1.781,0.636-2.033,0.551-1.152-0.402  c0.649-0.678,1.425-1.907,1.425-2.267c0-0.063-0.314,0.042-0.671,0.233c-0.377,0.212-1.215,0.53-1.844,0.72l-1.131,0.361l-1.027-0.7  c-0.566-0.381-1.361-0.805-1.781-0.932C39.766,21.902,38.131,21.944,37.167,22.283z M33,64C16.432,64,3,50.569,3,34S16.432,4,33,4  s30,13.431,30,30S49.568,64,33,64z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/></svg>

<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000" width="8px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>
</span></a>


<a href="https://www.instagram.com/m.hassaananwar/" target="_blank" class="link-transition instagram link dib z-999 pt3 pt0-l mr1" title="Instagram link" rel="noopener" aria-label="follow on Instagramâ€”â€”Opens in a new window">
  <svg height="32px"  style="enable-background:new 0 0 67 67;" version="1.1" viewBox="0 0 67 67" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M42.271,26.578v-0.006c0.502,0,1.005,0.01,1.508-0.002  c0.646-0.017,1.172-0.57,1.172-1.217c0-0.963,0-1.927,0-2.89c0-0.691-0.547-1.24-1.236-1.241c-0.961,0-1.922-0.001-2.883,0  c-0.688,0.001-1.236,0.552-1.236,1.243c-0.001,0.955-0.004,1.91,0.003,2.865c0.001,0.143,0.028,0.291,0.073,0.426  c0.173,0.508,0.639,0.82,1.209,0.823C41.344,26.579,41.808,26.578,42.271,26.578z M33,27.817c-3.384-0.002-6.135,2.721-6.182,6.089  c-0.049,3.46,2.72,6.201,6.04,6.272c3.454,0.074,6.248-2.686,6.321-6.043C39.254,30.675,36.462,27.815,33,27.817z M21.046,31.116  v0.082c0,4.515-0.001,9.03,0,13.545c0,0.649,0.562,1.208,1.212,1.208c7.16,0.001,14.319,0.001,21.479,0  c0.656,0,1.215-0.557,1.215-1.212c0.001-4.509,0-9.02,0-13.528v-0.094h-2.912c0.411,1.313,0.537,2.651,0.376,4.014  c-0.161,1.363-0.601,2.631-1.316,3.803s-1.644,2.145-2.779,2.918c-2.944,2.006-6.821,2.182-9.946,0.428  c-1.579-0.885-2.819-2.12-3.685-3.713c-1.289-2.373-1.495-4.865-0.739-7.451C22.983,31.116,22.021,31.116,21.046,31.116z   M45.205,49.255c0.159-0.026,0.318-0.049,0.475-0.083c1.246-0.265,2.264-1.304,2.508-2.557c0.025-0.137,0.045-0.273,0.067-0.409  V21.794c-0.021-0.133-0.04-0.268-0.065-0.401c-0.268-1.367-1.396-2.428-2.78-2.618c-0.058-0.007-0.113-0.02-0.17-0.03H20.761  c-0.147,0.027-0.296,0.047-0.441,0.08c-1.352,0.308-2.352,1.396-2.545,2.766c-0.008,0.057-0.02,0.114-0.029,0.171V46.24  c0.028,0.154,0.05,0.311,0.085,0.465c0.299,1.322,1.427,2.347,2.77,2.52c0.064,0.008,0.13,0.021,0.195,0.03H45.205z M33,64  C16.432,64,3,50.569,3,34S16.432,4,33,4s30,13.431,30,30S49.568,64,33,64z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/></svg>

<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000" width="8px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>
</span></a>




<a href="https://github.com/iamhassaan" target="_blank" class="link-transition github link dib z-999 pt3 pt0-l mr1" title="Github link" rel="noopener" aria-label="follow on Githubâ€”â€”Opens in a new window">
  <svg  height="32px"  style="enable-background:new 0 0 512 512;" version="1.1" viewBox="0 0 512 512" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
  <path d="M256,32C132.3,32,32,134.8,32,261.7c0,101.5,64.2,187.5,153.2,217.9c11.2,2.1,15.3-5,15.3-11.1   c0-5.5-0.2-19.9-0.3-39.1c-62.3,13.9-75.5-30.8-75.5-30.8c-10.2-26.5-24.9-33.6-24.9-33.6c-20.3-14.3,1.5-14,1.5-14   c22.5,1.6,34.3,23.7,34.3,23.7c20,35.1,52.4,25,65.2,19.1c2-14.8,7.8-25,14.2-30.7c-49.7-5.8-102-25.5-102-113.5   c0-25.1,8.7-45.6,23-61.6c-2.3-5.8-10-29.2,2.2-60.8c0,0,18.8-6.2,61.6,23.5c17.9-5.1,37-7.6,56.1-7.7c19,0.1,38.2,2.6,56.1,7.7   c42.8-29.7,61.5-23.5,61.5-23.5c12.2,31.6,4.5,55,2.2,60.8c14.3,16.1,23,36.6,23,61.6c0,88.2-52.4,107.6-102.3,113.3   c8,7.1,15.2,21.1,15.2,42.5c0,30.7-0.3,55.5-0.3,63c0,6.1,4,13.3,15.4,11C415.9,449.1,480,363.1,480,261.7   C480,134.8,379.7,32,256,32z"/>
</svg>

<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000" width="8px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>
</span></a>







    </div>
  </div>
</nav>

      <div class="tc-l pv6 ph3 ph4-ns">
        
          <h1 class="f2 f1-l fw2 white-90 mb0 lh-title">Walmart dataset analysis - An in-depth analysis on sales across several states using Machine Learning</h1>
          
            <h2 class="fw1 f5 f3-l white-80 measure-wide-l center lh-copy mt3 mb4">
              Walmart U.S dataset analysis - Predicting sales across states and stores. Analysis on a Kaggle Dataset (M5 Competition). This analysis was performed by me and my three group members : Gerardo Moreno, Jake Barrett and Jesus Sicairos Lopez
            </h2>
          
        
      </div>
    </div>
  </header>



    <main class="pb7" role="main">
      
  
  <article class="flex-l flex-wrap justify-between mw8 center ph3">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref b helvetica tracked">
          
        PROJECTS
      </aside>
      




  <div id="sharing" class="mt3">

    
    <a href="https://www.facebook.com/sharer.php?u=https://iamhassaan.github.io/My_Portfolio/post/project-1/" class="facebook no-underline" aria-label="share on Facebook">
      <svg height="32px"  style="enable-background:new 0 0 67 67;" version="1.1" viewBox="0 0 67 67" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M28.765,50.32h6.744V33.998h4.499l0.596-5.624h-5.095  l0.007-2.816c0-1.466,0.14-2.253,2.244-2.253h2.812V17.68h-4.5c-5.405,0-7.307,2.729-7.307,7.317v3.377h-3.369v5.625h3.369V50.32z   M33,64C16.432,64,3,50.569,3,34S16.432,4,33,4s30,13.431,30,30S49.568,64,33,64z" style="fill-rule:evenodd;clip-rule:evenodd;"/></svg>

    </a>

    
    
    <a href="https://twitter.com/share?url=https://iamhassaan.github.io/My_Portfolio/post/project-1/&amp;text=Walmart%20dataset%20analysis%20-%20An%20in-depth%20analysis%20on%20sales%20across%20several%20states%20using%20Machine%20Learning" class="twitter no-underline" aria-label="share on Twitter">
      <svg height="32px"  style="enable-background:new 0 0 67 67;" version="1.1" viewBox="0 0 67 67" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M37.167,22.283c-2.619,0.953-4.274,3.411-4.086,6.101  l0.063,1.038l-1.048-0.127c-3.813-0.487-7.145-2.139-9.974-4.915l-1.383-1.377l-0.356,1.017c-0.754,2.267-0.272,4.661,1.299,6.271  c0.838,0.89,0.649,1.017-0.796,0.487c-0.503-0.169-0.943-0.296-0.985-0.233c-0.146,0.149,0.356,2.076,0.754,2.839  c0.545,1.06,1.655,2.097,2.871,2.712l1.027,0.487l-1.215,0.021c-1.173,0-1.215,0.021-1.089,0.467  c0.419,1.377,2.074,2.839,3.918,3.475l1.299,0.444l-1.131,0.678c-1.676,0.976-3.646,1.526-5.616,1.568  C19.775,43.256,19,43.341,19,43.405c0,0.211,2.557,1.397,4.044,1.864c4.463,1.377,9.765,0.783,13.746-1.568  c2.829-1.673,5.657-5,6.978-8.221c0.713-1.716,1.425-4.851,1.425-6.354c0-0.975,0.063-1.102,1.236-2.267  c0.692-0.678,1.341-1.419,1.467-1.631c0.21-0.403,0.188-0.403-0.88-0.043c-1.781,0.636-2.033,0.551-1.152-0.402  c0.649-0.678,1.425-1.907,1.425-2.267c0-0.063-0.314,0.042-0.671,0.233c-0.377,0.212-1.215,0.53-1.844,0.72l-1.131,0.361l-1.027-0.7  c-0.566-0.381-1.361-0.805-1.781-0.932C39.766,21.902,38.131,21.944,37.167,22.283z M33,64C16.432,64,3,50.569,3,34S16.432,4,33,4  s30,13.431,30,30S49.568,64,33,64z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/></svg>

    </a>

    
    <a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://iamhassaan.github.io/My_Portfolio/post/project-1/&amp;title=Walmart%20dataset%20analysis%20-%20An%20in-depth%20analysis%20on%20sales%20across%20several%20states%20using%20Machine%20Learning" class="linkedin no-underline" aria-label="share on LinkedIn">
      <svg  height="32px"  style="enable-background:new 0 0 65 65;" version="1.1" viewBox="0 0 65 65" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
  <path d="M50.837,48.137V36.425c0-6.275-3.35-9.195-7.816-9.195  c-3.604,0-5.219,1.983-6.119,3.374V27.71h-6.79c0.09,1.917,0,20.427,0,20.427h6.79V36.729c0-0.609,0.044-1.219,0.224-1.655  c0.49-1.22,1.607-2.483,3.482-2.483c2.458,0,3.44,1.873,3.44,4.618v10.929H50.837z M22.959,24.922c2.367,0,3.842-1.57,3.842-3.531  c-0.044-2.003-1.475-3.528-3.797-3.528s-3.841,1.524-3.841,3.528c0,1.961,1.474,3.531,3.753,3.531H22.959z M34,64  C17.432,64,4,50.568,4,34C4,17.431,17.432,4,34,4s30,13.431,30,30C64,50.568,50.568,64,34,64z M26.354,48.137V27.71h-6.789v20.427  H26.354z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>

    </a>
  </div>


      <h1 class="f1 athelas mt3 mb1">Walmart dataset analysis - An in-depth analysis on sales across several states using Machine Learning</h1>
      
      
      <time class="f6 mv4 dib tracked" datetime="2020-02-09T10:58:08-04:00">February 9, 2020</time>

      
      
    </header>
    <div class="nested-copy-line-height lh-copy serif f4 nested-links nested-img mid-gray pr4-l w-two-thirds-l"><p>In this project, we cover some of the time series methods used in past competitions [7][8][9] to investigate the dataset provided by Walmart.</p>
<p>First, we build a special kind of RNN model called LSTM which attempts to predict the unit sales of individual stores. This is done by considering a univariate model and then using a more complex multivariate model, which we hope increases the test accuracy. Next, from the scikit-learn library in Python, we build an RF model which analyzes the item HOUSEHOLD_1_272_CA_3_validation, whilst selecting the most important features which help predict the sales of this particular product. We also compare the Random Forest (RF) results with other scikit-learn models. Lastly, we use an ARIMA model to predict the unit sales of the category HOBBIES whilst considering the fact that the dataset may not be stationary and pre-processing may be required before making progress.</p>
<p>Data Management:</p>
<p>Walmart created the files for the M5 competition sales_train_validation.csv, calendar.csv and sell_prices.csv.
The first contains the daily unit sales for a 3,049 products over the last 1,913 days and the date of the first day being 2011-01-29 and the last being 2016-06-19. Each product is categorized by one of 7 departments, 3 categories, 10 stores and 3 states. The second contains columns describing the dates and events for each day a product is sold. The last contains information regarding the price of each item for each store on a certain day.</p>
<p>Exploration:</p>
<p>Our investigation for the RNN model began by looking at the number of unit sales per day for each Walmart store. We imported the dataset into Python as a pandas data frame. Then, we dropped the redundant columns from the original dataset and summed the unit sales per day and grouped them by store_id.</p>
<p><figure>
    <img src="/My_Portfolio/images/aggrsales.png"/> 
</figure>
.</p>
<p>Next, we produced separate plots for stores based on the state theyâ€™re located in. The idea was to visually identify whether stores from the same state follow any obvious trend to justify univariate and multivariate models. Weâ€™d then go on to see whether introducing multiple variables has any significant impact on our predictions.</p>
<p><figure>
    <img src="/My_Portfolio/images/subplots.png"/> 
</figure>
.</p>
<p><figure>
    <img src="/My_Portfolio/images/sub2.png"/> 
</figure>
.</p>
<p>We also explored, for the RF model, the top 10 highest revenue items in Walmart stores for each category and state through the entire history of the dataset. As in Table III, the product FOODS_3_090_CA_3_validation is the most famous food item in CA with total dollar sales of $4988.90,</p>
<p><figure>
    <img src="/My_Portfolio/images/revenue.png"/> 
</figure>
.</p>
<p>Subsequently, we looked at the time series prediction of HOUSEHOLD_1_272_CA_3_validation. We stored the daily sales of this item throughout the timeline in a data frame and transposed the matrix. Then, we joined this item to calendar.csv to get the details of events happening on each day which gave us flexibility in answering questions like why sales in one day was greater than another.</p>
<p>High loss is generally caused by models that are unable to learn from the training data. This means we have either used too few features such that the model is unable to map the data points causing under-fitting or the model is memorizing the mappings such that it may behave poorly on unseen data causing over-fitting. In our case, since we have around 60 features to train on, weâ€™re likely to overfit. In Table III, weâ€™ve created an importance table that allows us to see the importance of each variable in our dataset. The results suggest that variable ğ‘‘ is most important, representing the day, followed by snap_CA, snap_TX and snap_WI. The most important years were 2011, 2014 and the most important months were 3, 1 (March and January).</p>
<p><figure>
    <img src="/My_Portfolio/images/imp.png"/> 
</figure>
.</p>
<p>The goal for the ARIMA model was to predict the sales a store makes for products that belong to a specific category; in our case we base our focus on the HOBBIES category of CA_1. We started by loading the data into a pandas data frame and grouped our data by store and then by category. This was possible with the help of the pandas group_by function. Lastly, we added all the values for each day in each group.</p>
<p><figure>
    <img src="/My_Portfolio/images/hobb.png"/> 
</figure>
.</p>
<p>Methodology:</p>
<ol>
<li>RNN:</li>
</ol>
<p>We used RNNs in this project as theyâ€™re able to capture the patterns of multivariate input data which is suitable for the task weâ€™re undertaking [1]. They can easily handle sequential data like ours and it considers the previously received inputs, unlike a typical Feed Forward neural network. We use TensorFlow with the Keras RNN API, which is designed for modelling sequential data for time series problems. We utilize a particular kind of RNN network called LSTM.</p>
<ol start="2">
<li>RF:</li>
</ol>
<p>To complement the competence of RFs over other machine learning models as mentioned in Section III, we constructed a comparison chart evaluating the performance of scikit-learn models on our training and test sets. To make the predictions, we split 80% of the data for training on scikit-learnâ€™s Random Forest Regressor. We used 1,000 estimators with random state set to zero. After fitting the model, we obtained an MAE of 5.85 on our test set. For simplicity, weâ€™re representing just 3 layers of the tree.</p>
<p><figure>
    <img src="/My_Portfolio/images/rf.png"/> 
</figure>
.</p>
<ol start="3">
<li>Arima:</li>
</ol>
<p>that information in the past of a time series can be sufficient to accurately predict values in the future. The model has the general notation ARIMA(ğ‘,ğ‘‘,ğ‘), where ğ‘ is the order of Auto Regressive term and refers to the number of time lags ğ‘¡ to be used as predictors, ğ‘‘ is the minimum number of differencing required to make a non-stationary series stationary and ğ‘ is the order of the Moving Average term indicating the required number of lagged forecast errors. It can be represented by the following equation:
ğœ™(ğ¿)(1âˆ’ğ¿)ğ‘‘ğ‘‹ğ‘¡=ğœ‡+ğ›©(ğ¿)ğœ–ğ‘¡, (ï€±) where ğœ–ğ‘¡ is the white noise distribution process ğ‘Šğ‘(0,ğœ2) and ğ¿ is the backward-shift operator with:
ğœ™(ğ¿)=1âˆ’ğœ™1ğ¿âˆ’â‹¯âˆ’ğœ™ğ‘ğ¿ğ‘, (ï€²)
ğ›©(ğ¿)=1âˆ’ğ›©1ğ¿+â‹¯+ğ›©ğ‘ğ¿ğ‘, (ï€³)
with ğœ™ğ‘,ğ›©ğ‘â‰ 0.</p>
<p>Testing and Results:</p>
<ol>
<li>Univariate Model:</li>
</ol>
<p>We introduce a simple LSTM model with a single 8-unit LSTM layer for a single-step prediction. We shuffle the tensors, cache the dataset and use batches of size 128 to propagate through the network so that training is faster and the memory requirement is less. We use a stochastic gradient descent optimizer which simply computes:
ğœƒğ‘¡+1=ğœŒğœƒğ‘¡âˆ’ğ›¼ğ›»ğœƒ
where ğœƒ is the weight vector at a specified time, ğ‘¡ is the time being a certain day, ğœŒis the momentum which helps accelerate the gradient descent process to achieve faster convergence, ğ›¼ is the LR of the optimizer and ğ›»ğœƒ is the gradient with given weights. Over 100 epochs, we train our model and experiment with the LR using values 0.01 and 0.05.</p>
<p><figure>
    <img src="/My_Portfolio/images/ttable.png"/> 
</figure>
.</p>
<p>Itâ€™s common practice to rescale the dataset before training a neural network [16] to help avoid the problem of getting stuck at local optima.The loss function we use is MAE. We see the best prediction for the univariate single-step model come from store TX_1 with LR 0.05.</p>
<p><figure>
    <img src="/My_Portfolio/images/losses.png"/> 
</figure>
.</p>
<p>We also introduce multiple steps into the univariate time series where we attempt to predict 20 future steps. Since this is a more complex task, we add a 16-unit LSTM layer, with the original LSTM layer increased to 32 and the dense layer to 20. We see the best prediction comes from CA_1 and with LR 0.05.</p>
<p><figure>
    <img src="/My_Portfolio/images/degradation.png"/> 
</figure>
.</p>
<ol start="2">
<li>Multivariate Model:</li>
</ol>
<p>A) LSTM:</p>
<p>Since the multivariate task is a higher order of complexity, for the single-step model, we increase the number of units in the LSTM from 8 to 32. All other parameters of the model remain, with the only experimental adjustment being the LR. When doing the multivariate prediction for a store, we only use the data in stores which belong in the same state, meaning when we make a prediction on the unit sales of TX_1, we only consider the data in stores TX_1, TX_2 and TX_3. For the multivariate, multistep model, we adopt the same architecture as the single-step.</p>
<p><figure>
    <img src="/My_Portfolio/images/multivariate.png"/> 
</figure>
.</p>
<p><figure>
    <img src="/My_Portfolio/images/multi2.png"/> 
</figure>
.</p>
<p>From the results in Table VIII, we find for the multi-step models that decreasing the LR generally leads to improvement in predictive performance on each store. It is unclear whether this improvement exists with the single-step models. In fact, the univariate model favors the higher LR. We see in the single-step case, for 9 out of 10 stores, the univariate model achieves the lowest percentage error (over LR 0.01 or 0.05) than the multivariate model, which may mean weâ€™re overcomplicating matters by fitting redundant features. This cannot be said for the multi-step case, where 5 out of 10 stores for the multivariate model perform better than their univariate counterpart. This leads us to believe that for multi-step, for some stores, we under-fit the data by oversimplying the problem using a univariate model but we manage to capture the trend of the data better by using a more complex multivariate model.</p>
<p>B) Random Forest:</p>
<p>We set a threshold of 0.2, so that the variables which have importance factors greater than 0.2 will be considered for analysis as in Table III. We analysed the item HOUSEHOLD_1_272_CA_3_validation and when we fitted our model, we realized that reducing the number of features had a bad impact on our MAE, causing it to increase to 11.44.</p>
<p><figure>
    <img src="/My_Portfolio/images/first.png"/> 
</figure>
.</p>
<p>To improve our results, we used several techniques:
Data Scaling:
The input features were scaled using Sklearn with the MAE being 8.49 after fitting the model and calculating predictive labels.
Feature Selection. To increase the performance, we chose another group of features now with â€œsnap_CAâ€, â€œsnap_TXâ€, â€œsnap_WIâ€, â€œChanukah Endâ€, â€œChristmasâ€, â€œColumbus Dayâ€,â€ LentStart,â€ ,â€œVeteransDayâ€, â€œDay of the weekâ€, â€œmonthâ€ and â€œyearâ€. The month, year and day of the week were all one-hot encoded.
Test Split Sizes.</p>
<p>a. Test Split 20%
The data frame we obtained after feature selection step had 35 columns. The test size was set to 20%, so that 80 % of the data should be used for training. We saw a decrease of the MAE to 5.94.
<figure>
    <img src="/My_Portfolio/images/firsttest.png"/> 
</figure>
.</p>
<p>b. Test Split 1%:</p>
<p>Here, we took a test split of 0.01, meaning that 99% of the data would be used for training. The result obtained was far better than the previous case with test MAE of 4.33. The results indicate that, with greater test split, weâ€™re compromising on our training quality since less data is available for training. This is shown through the decrease of the MAE when we decreased the test split.
<figure>
    <img src="/My_Portfolio/images/secondtest.png"/> 
</figure>
.</p>
<p>Comparing with other Scikit Models. We were aware that other models in scikit-learn could potentially perform better than RF. To check this, we evaluated the performance of all models in scikit-learn library against our data as in Table IX.</p>
<p><figure>
    <img src="/My_Portfolio/images/scikitlearn.png"/> 
</figure>
.</p>
<p>Bayesian Ridge Regression (BRR). Itâ€™s evident that BRR is one of the best performing models. To further understand how this model performs on our dataset, we decided to visualize its performance on different test splits.</p>
<p>a. Test Split 20%:
Using BRR with a test split of 0.2, we obtain an MAE of 5.84.</p>
<p><figure>
    <img src="/My_Portfolio/images/bayesian1.png"/> 
</figure>
.</p>
<p>b. Test Split 1%
Now, using a test split of 0.01, our results are better. We obtain an MAE of 3.90 which is a significant performance increase compared with the same split for the RF prediction.
Experiments clearly show that BRR and RFs both need more data for reliable predictions. If we try to increase the test split, this will give the model less data to train on, making the predictive performance worse but having a higher certainty of it. If we decrease the test split, weâ€™ll get a better prediction due to more training data but higher variance results for unseen data.
<figure>
    <img src="/My_Portfolio/images/bayesian2.png"/> 
</figure>
.</p>
<p>C. ARIMA:</p>
<p>We first checked whether the dataset was in fact stationary. In Fig. 20, we found that it wasnâ€™t stationary since the bars after time-lag 0 donâ€™t lie within the critical interval. To fix this, we used differencing and depending on the complexity of the series, it may be needed more than once, as in Fig. 21. We made use of a built-in function called diff and were able to implement our ARIMA model to make some predictions.</p>
<p><figure>
    <img src="/My_Portfolio/images/arima1.png"/> 
</figure>
.</p>
<p>We then use the ARIMA function that comes with the statsmodels.tsa.arima_model package. We used only 365 days of historic data, since ARIMA models tend to perform better in shorter timeframes. We split the 365 days into training and testing sets, approximately 90% and 10% respectively. To train the model, we send the train data as a parameter and specify each parameter required for the ARIMA(ğ‘,ğ‘‘,ğ‘) model. After this, we fit our model by calling the fit function on our trained model. To make predictions, we call the function forecast on our fitted model and specify the number of predictions we want to make. In our case, we set that number to 24 with our prediction shifted to better fit the data.</p>
<p><figure>
    <img src="/My_Portfolio/images/arima2.png"/> 
</figure>
.</p>
<p>We observe that our model is able to capture the pattern of the data to some extent. The MSE obtained with this approach was 239.97. So, we decided to implement another model that works in a similar way to see if we could lower that value. We opted for the autoregressive (AR) model. The process is similar to the ARIMA model with the only difference being the training of the model. We use the AR function instead and we only send the training data as a parameter. Again, we fit our model and finally we make predictions by calling the predict function on the fitted model.</p>
<p><figure>
    <img src="/My_Portfolio/images/arima3.png"/> 
</figure>
.</p>
<p>Limitations: We found that with building deeper RNNs, computational complexity increases and therefore so does training time or the resources required. ARIMA required a fair amount of preprocessing of the data in order to eventually build a model fit for training, specifically with differencing to ensure the model is stationary. We found with RF, finding the best split for reliable training yet good test accuracy proved to be a challenge.</p>
<p>Future work. If we could do further analysis, we would utilize Googleâ€™s TPUs further to build a deeper, more complex RNN model since time-series forecasting is not a simple task. Itâ€™s unclear from our results that introducing a more complex multivariate model improves the performance. It would be interesting to see if a more complex model than the one we implemented would improve performance.
However, a deep model isnâ€™t necessarily a better one [10]. Moreover, we couldâ€™ve tried a hybrid model, as Smyl in the M4 competition â€œwhich blended exponential smoothing and ML concepts into a single model of remarkable accuracyâ€ [9] In the M4 competition paper, discussed is the â€œbest model versus combining against single methodsâ€ where Makridakis et al. consistently find higher accuracy from combining than from a single model. Combining methods together produces diversity within a model, which helps cancel out random noise, thus leading to improved performance. Hence, combining RF, ARIMA and RNN could lead to even better results.</p>
<p>REFERENCES:
[1] H.Hewamalage et al., â€œRecurrent Neural Networks for Time Series Forecasting: Current Status and Future Directions.â€
[2] S.Hochreiter et al., â€œFlat Minima,â€ LONG SHORT-TERM MEMORY, Jan.1997.
[3] A.Hunt et al., The pragmatic programmer: from journeyman to master. Boston:Addison-Wesley, 2015.
[4] M.J.Kane et al., â€œComparison of ARIMA and Random Forest time series models for prediction of avian influenza H5N1 outbreaks,â€ BMC Bioinformatics, Aug.2014. [5] Y.LeCun et al., â€œDeep learning,â€ Nature, May.2015.
[6] S.Makridakis et al., â€œThe accuracy of extrapolation (time series) methods: Results of a forecasting competition,â€ Journal of Forecasting, Apr.1982.
[7] S.Makridakis et al., â€œThe M2-competition: A real-time judgmentally based forecasting study,â€ International Journal of Forecasting, Apr.1993.
[8] S.Makridakis et al., â€œThe M3-Competition: results, conclusions and implications,â€ International Journal of Forecasting, 2000.
[9] S.Makridakis et al., â€œThe M4 Competition: 100,000 time series and 61 forecasting methods,â€ International Journal of Forecasting, Jan.2020.
[10] E.Malach et al., â€œIs Deeper Better only when Shallow is Good?,â€ Mar.2019.
[11] J.N.K.Rao et al., â€œTime Series Analysis Forecasting and Control,â€ Sep.1972.
[12] A.Redd et al., â€œFast ES-RNN: A GPU Implementation of the ES-RNN Algorithm.â€
[13] V.Svetnik, et al., â€œRandom Forest: A Classification and Regression Tool for Compound Classification and QSAR Modeling,â€ Journal of Chemical Information and Computer Sciences, Nov.2003.
[14] Y.Takefuji et al., â€œEffectiveness of ensemble machine learning over the conventional multivariable linear regression models,â€ Jan.2016.
[15] H.Tanaka, â€œFuzzy data analysis by possibilistic linear models,â€ Fuzzy Sets and Systems, Dec.1987.
[16] W.Williams et al., â€œSCALING RECURRENT NEURAL NETWORK LANGUAGE MODELS.â€</p>
<p>Code Implementation
We used notebooks to create our well-documented codes so that it is easy for the reader to follow. They were used to generate the results for each model and can be found on our GitHub repository:
<a href="https://github.com/GerardoMoreno96/ECS784P_DataAnalyticsProject">https://github.com/GerardoMoreno96/ECS784P_DataAnalyticsProject</a></p>
<p>Dataset:</p>
<p><figure>
    <img src="/My_Portfolio/images/dataset.png"/> 
</figure>
.</p>
<ul class="pa0">
  
   <li class="list">
     <a href="/My_Portfolio/tags/scene" class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">scene</a>
   </li>
  
</ul>
<div class="mt6 instapaper_ignoref">
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




  <div class="bg-light-gray pa3 nested-list-reset nested-copy-line-height nested-links">
    <p class="f5 b mb3">Related</p>
    <ul class="pa0 list">
	   
	     <li  class="mb2">
          <a href="/My_Portfolio/post/project-3/">Flask Restful API for a Spanish Store</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/My_Portfolio/post/project-4/">SRSCP- Semantic Reasoner for Smart Car Purchases</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/My_Portfolio/post/project-5/">GMM Clustering in Peterson and Barney Dataset&#34;</a>
        </li>
	    
    </ul>
</div>

</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="https://iamhassaan.github.io/My_Portfolio" >
    &copy;  Muhammad Hassaan Anwar 2020 
  </a>
    <div>




<a href="https://twitter.com/iamthehassaan" target="_blank" class="link-transition twitter link dib z-999 pt3 pt0-l mr1" title="Twitter link" rel="noopener" aria-label="follow on Twitterâ€”â€”Opens in a new window">
  <svg height="32px"  style="enable-background:new 0 0 67 67;" version="1.1" viewBox="0 0 67 67" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M37.167,22.283c-2.619,0.953-4.274,3.411-4.086,6.101  l0.063,1.038l-1.048-0.127c-3.813-0.487-7.145-2.139-9.974-4.915l-1.383-1.377l-0.356,1.017c-0.754,2.267-0.272,4.661,1.299,6.271  c0.838,0.89,0.649,1.017-0.796,0.487c-0.503-0.169-0.943-0.296-0.985-0.233c-0.146,0.149,0.356,2.076,0.754,2.839  c0.545,1.06,1.655,2.097,2.871,2.712l1.027,0.487l-1.215,0.021c-1.173,0-1.215,0.021-1.089,0.467  c0.419,1.377,2.074,2.839,3.918,3.475l1.299,0.444l-1.131,0.678c-1.676,0.976-3.646,1.526-5.616,1.568  C19.775,43.256,19,43.341,19,43.405c0,0.211,2.557,1.397,4.044,1.864c4.463,1.377,9.765,0.783,13.746-1.568  c2.829-1.673,5.657-5,6.978-8.221c0.713-1.716,1.425-4.851,1.425-6.354c0-0.975,0.063-1.102,1.236-2.267  c0.692-0.678,1.341-1.419,1.467-1.631c0.21-0.403,0.188-0.403-0.88-0.043c-1.781,0.636-2.033,0.551-1.152-0.402  c0.649-0.678,1.425-1.907,1.425-2.267c0-0.063-0.314,0.042-0.671,0.233c-0.377,0.212-1.215,0.53-1.844,0.72l-1.131,0.361l-1.027-0.7  c-0.566-0.381-1.361-0.805-1.781-0.932C39.766,21.902,38.131,21.944,37.167,22.283z M33,64C16.432,64,3,50.569,3,34S16.432,4,33,4  s30,13.431,30,30S49.568,64,33,64z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/></svg>

<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000" width="8px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>
</span></a>


<a href="https://www.instagram.com/m.hassaananwar/" target="_blank" class="link-transition instagram link dib z-999 pt3 pt0-l mr1" title="Instagram link" rel="noopener" aria-label="follow on Instagramâ€”â€”Opens in a new window">
  <svg height="32px"  style="enable-background:new 0 0 67 67;" version="1.1" viewBox="0 0 67 67" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M42.271,26.578v-0.006c0.502,0,1.005,0.01,1.508-0.002  c0.646-0.017,1.172-0.57,1.172-1.217c0-0.963,0-1.927,0-2.89c0-0.691-0.547-1.24-1.236-1.241c-0.961,0-1.922-0.001-2.883,0  c-0.688,0.001-1.236,0.552-1.236,1.243c-0.001,0.955-0.004,1.91,0.003,2.865c0.001,0.143,0.028,0.291,0.073,0.426  c0.173,0.508,0.639,0.82,1.209,0.823C41.344,26.579,41.808,26.578,42.271,26.578z M33,27.817c-3.384-0.002-6.135,2.721-6.182,6.089  c-0.049,3.46,2.72,6.201,6.04,6.272c3.454,0.074,6.248-2.686,6.321-6.043C39.254,30.675,36.462,27.815,33,27.817z M21.046,31.116  v0.082c0,4.515-0.001,9.03,0,13.545c0,0.649,0.562,1.208,1.212,1.208c7.16,0.001,14.319,0.001,21.479,0  c0.656,0,1.215-0.557,1.215-1.212c0.001-4.509,0-9.02,0-13.528v-0.094h-2.912c0.411,1.313,0.537,2.651,0.376,4.014  c-0.161,1.363-0.601,2.631-1.316,3.803s-1.644,2.145-2.779,2.918c-2.944,2.006-6.821,2.182-9.946,0.428  c-1.579-0.885-2.819-2.12-3.685-3.713c-1.289-2.373-1.495-4.865-0.739-7.451C22.983,31.116,22.021,31.116,21.046,31.116z   M45.205,49.255c0.159-0.026,0.318-0.049,0.475-0.083c1.246-0.265,2.264-1.304,2.508-2.557c0.025-0.137,0.045-0.273,0.067-0.409  V21.794c-0.021-0.133-0.04-0.268-0.065-0.401c-0.268-1.367-1.396-2.428-2.78-2.618c-0.058-0.007-0.113-0.02-0.17-0.03H20.761  c-0.147,0.027-0.296,0.047-0.441,0.08c-1.352,0.308-2.352,1.396-2.545,2.766c-0.008,0.057-0.02,0.114-0.029,0.171V46.24  c0.028,0.154,0.05,0.311,0.085,0.465c0.299,1.322,1.427,2.347,2.77,2.52c0.064,0.008,0.13,0.021,0.195,0.03H45.205z M33,64  C16.432,64,3,50.569,3,34S16.432,4,33,4s30,13.431,30,30S49.568,64,33,64z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/></svg>

<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000" width="8px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>
</span></a>




<a href="https://github.com/iamhassaan" target="_blank" class="link-transition github link dib z-999 pt3 pt0-l mr1" title="Github link" rel="noopener" aria-label="follow on Githubâ€”â€”Opens in a new window">
  <svg  height="32px"  style="enable-background:new 0 0 512 512;" version="1.1" viewBox="0 0 512 512" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
  <path d="M256,32C132.3,32,32,134.8,32,261.7c0,101.5,64.2,187.5,153.2,217.9c11.2,2.1,15.3-5,15.3-11.1   c0-5.5-0.2-19.9-0.3-39.1c-62.3,13.9-75.5-30.8-75.5-30.8c-10.2-26.5-24.9-33.6-24.9-33.6c-20.3-14.3,1.5-14,1.5-14   c22.5,1.6,34.3,23.7,34.3,23.7c20,35.1,52.4,25,65.2,19.1c2-14.8,7.8-25,14.2-30.7c-49.7-5.8-102-25.5-102-113.5   c0-25.1,8.7-45.6,23-61.6c-2.3-5.8-10-29.2,2.2-60.8c0,0,18.8-6.2,61.6,23.5c17.9-5.1,37-7.6,56.1-7.7c19,0.1,38.2,2.6,56.1,7.7   c42.8-29.7,61.5-23.5,61.5-23.5c12.2,31.6,4.5,55,2.2,60.8c14.3,16.1,23,36.6,23,61.6c0,88.2-52.4,107.6-102.3,113.3   c8,7.1,15.2,21.1,15.2,42.5c0,30.7-0.3,55.5-0.3,63c0,6.1,4,13.3,15.4,11C415.9,449.1,480,363.1,480,261.7   C480,134.8,379.7,32,256,32z"/>
</svg>

<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000" width="8px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>
</span></a>






</div>
  </div>
</footer>

    

  <script src="/My_Portfolio/dist/js/app.3fc0f988d21662902933.js"></script>


  </body>
</html>
